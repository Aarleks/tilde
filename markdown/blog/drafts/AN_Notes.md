# Notes for RD1 LG 1H 2021

## 1.4 Updates to RD1 in 1H 2021

Researcher Development 1 - or what we refer to as RD1 - has evolved over the years to become a unit primarily intended to kick start your development as a researcher. When the first iteration of the unit was offered in 2015, it was called 'Research Design 1: Theories of Enquiry'. This somewhat esoteric name came from the notion that there could be a central unit in which students from all disciplines could be taught how to design a research project. This is, of course, a preposterous idea; the diversity of ways humans try to know about the universe is far to great to be able to fit into a single unit of study. And what would students from astrophysics do while the teaching was about ficto-critical methods? Or the students from sociology while we discussed the merits of the Torey-Bloch equation in Nuclear Magnetic Resonance Imaging. It's kind of silly. And we began to take what we had been given and turn it into something that would work - a unit that students commencing an MRes (or any graduate degree, for that matter), could use to begin developing themselves as professional researchers.

To do this, we - my colleague Dr Jack Tsonis and I - began to read and think about what research training was said to be, what it actually was in reality, and what it needed to be. We reflected that often research training simply consisted of 'figuring it out for yourself'. This didn't strike us as a sensible solution to the problem of ensuring the continuation of the human capacity to produce reliable knowledge.

From that reflection, a question grew in our minds; 'What do we wish we had been taught about research as a career choice?' Nobody had ever explicitly taught us - like everybody else, we had had to figure it out as we went, for the most part. We also realised that when we began to think about what research was, what its constituent parts were in the form of tasks and skills, that at the most basic level research was the process of managing information and communicating it. We spoke with our colleagues around the world, across many disciplines, and realised that their answers boiled down to three main things: how to read, how to write, and what the business of research really looked like.

So, over the course of 2018-2019, that is what we turned the unit into. For the most part, the aim of the unit is 'like it says on the tin': to help you learn about reading and writing in the context of research, and to introduce you to and get you thinking about the professional world of research.

The new unit - Researcher Development 1: Reading, Writing, and the Business of Research - debuted in February 2020.

### Sat/Unsat

In late 2017 I read an article that changed me as a teacher. I had, for many years, found assigning a percentage mark (or a number) to a piece of qualitative work, such as an essay, deeply problematic. What was an 81% essay compared with an 82% one? If the difference warranted the mark it must be observable to anyone with appropriate training and, moreover, the two observers ought to come to the same marks with some measure of reliability. I had developed my thinking on the latter after reading and researching qualitative methods for analysing interview data. Specifically, the notion of 'inter-rater reliability' struck me as highly salient for teaching, but in research it was slippery and difficult. The problem, from a reliability point of view, was not that different raters would come to wildly different scores, but they would only reliably achieve similar scores. This effect was amplified with the granularity of the rating. For instance, it is much easier to get very close inter-rater reliability on a score between 1-5. It is much harder to get reliability on a score between 1-100.

What Jack and I found was that, after a couple of years working together, our inter-rater reliability meant we would reliably arrive at marks +/- 2-3 marks out of 100. In other words, if I gave a piece of work a mark of 70/100, I was pretty sure Jack would give it somewhere between 68-72. The point I am getting at is that for qualitative, writing-based assessments, marks out of 100 are, at best, pointless. More likely they are harmful to student learning, and to students because of the way they focus attention on the work, not the worker. And, much more problematically, they are a lie - marks out of 100 purport to students and to ourselves that there is a real and consistently identifiable difference between an essay marked 70 and one marked 71. There isn't. Or at least, there isn't when one is marking in the same way one reads normally. If one were to mark an essay by counting grammatical errors, formatting aberrations, or counting the number of interesting or useful sources cited... sure... maybe you could come up with a relatively objective system for arriving at at integer. But that's not the point of essay work for the purposes of learning.

When we set essays, what we are usually setting is not a test but a learning activity.

The best Jack and I could get to, after years working together marking the same assessments and meeting multiple times every semester to develop inter-rater reliability was +/- 2-3%. To me, this says that percentage grades for essay work are silly, and that letter grades (A, B, C, etc.) or qualitative grades (High Distinction, Distinction, Credit, etc.) are as good as we can get.I would also say, however, that most grades serve little to no pedagogical purpose for the student. Grades are mostly for institutions. In fact, in general, grades make education harder.
